"""
LLM_Benchmarking__
|
constants.py
Created on Mon Jun  9 15:36:52 2025
@authors: Rochana Obadage, Bang Nguyen
"""

import os
from dotenv import load_dotenv

load_dotenv()

API_KEY = os.getenv("API_KEY")


TEMPLATE_PATHS = {
    "post_registration_template": "templates/post_registration_schema.json",
    "pre_registration_template": "templates/pre_registration_schema.json",
    "info_extractor_instructions": "templates/info_extractor_instructions.json",
    "extract_eval_prompt_template": "templates/prompts/extract_eval.txt",
    "generate_design_eval_prompt_template": "templates/prompts/generate_design_eval.txt"
}



GENERATE_REACT_CONSTANTS = {
    "files": {
        "original_paper.pdf": "The pdf file containing the full text of the original paper",
        "initial_details.txt": "Details about the claim from the original paper to be replicated",
        "post_registration.json": "A structured document with key extracted information about the original paper and the claim to be replicated.",
        "replication_data": "The folder containing the data that can potentially be used for the replication. There may also be useful code to help you with the replication. But if not, you have to generate the replication code yourself in Python.",
    },
    "json_template_python": "templates/pre_registration_schema_python.json",
    "json_template_native": "templates/pre_registration_schema_native.json"
}

GENERATE_EXECUTE_REACT_CONSTANTS = {
    "files": {
        "original_paper.pdf": "The pdf file containing the full text of the original paper",
        "initial_details.txt": "Details about the claim from the original paper to be replicated",
        "post_registration.json": "A structured document with key extracted information about the original paper and the claim to be replicated.",
        "replication_preregistration.json": "A structured document with plans for your replication of the claim.",
        "replication_data_code": "The folder containing the data and code that can be used for the replication.",
    },
    "json_template": "templates/execute_schema.json"
}

EVALUATE_GENERATE_EXECUTE_CONSTANTS = {
    "prompt_template": "templates/prompts/execute_eval.txt",
    "claim_files": {
        "original_paper.pdf": "The pdf file containing the full text of the original paper",
        "initial_details.txt": "Details about the claim from the original paper to be replicated",
        "replication_data": "The folder containing the data that can potentially be used for the replication. There may also be useful code to help you with the replication. But if not, you have to generate the replication code yourself in Python.",
    },
    "agent_files": {
        "post_registration.json": "A structured document with key extracted information about the original paper and the claim to be replicated.",
        "_log/": "Folder contain the logs of the agent replication attempt. Focus on the logs of the design and the execute stage for this evaluation.",
        "replication_info.json": "Final structured report of the design stage by the agent.",
        "execution_results.json": "Final strcuterd report of the execution stage by the agent."
    },
    "json_template": "templates/evaluate_execute_schema.json"
}


INTERPRET_CONSTANTS = {
    "prompt_template": "templates/prompts/interpret.txt",
    "claim_files": {
        "original_paper.pdf": "The pdf file containing the full text of the original paper",
        "initial_details.txt": "Details about the claim from the original paper to be replicated",
    },
    "agent_files": {
        "post_registration.json": "A structured document with key extracted information about the original paper and the claim to be replicated.",
        "replication_info.json": "Structured report of the agent at the PLANNING stage for the replication of the given claim.",
        "replication_data": "The folder containing the data and code that were used for the replication, along with any output files generated after running the code. You MUST examine any additional execution result files not reported in execution_results.json before making your interpretataions.",
        "execution_results.json": "Final structured report of the execution stage by the agent. If the report doesn't have any results, look for output files generated by the code to find the execution results before making conclusions.",
    },
    "json_template": "templates/interpret_schema.json"
}

EVALUATE_INTERPRET_CONSTANTS = {
    "prompt_template": "templates/prompts/interpret_eval.txt",
    "interpret_results": "interpret_results.json",
    "json_template": "templates/interpret_schema.json"
}

